{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c66d9db",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "---\n",
    "- 패키지 설치\n",
    "    - NLTK: pip install nltk\n",
    "    - Konply : pip install konply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7fa7b7",
   "metadata": {},
   "source": [
    "## [1] 토큰화(Tokenization)\n",
    "---\n",
    "- 문장/문서를 의미르 지닌 작은 단위로 나누는 것\n",
    "- 나누어진 단어를 토큰(Token)이라 함\n",
    "- 종류\n",
    "    - 문장 토큰화\n",
    "    - 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88fb0fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb4f855b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk Corpus(말뭉치) 데이터셋 다운받기\n",
    "nltk.download('all', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f62e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text1='''\n",
    "In Korea, more than half of residents speak Korean?\n",
    "GitHub Actions makes it easy to automate? \n",
    "all your software workflows!\n",
    "'''\n",
    "raw_text2='''\n",
    "GitHub Actions makes it easy to automate all your software workflows \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66731fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'Korea', ',', 'more', 'than', 'half', 'of', 'residents', 'speak', 'Korean', '?', 'GitHub', 'Actions', 'makes', 'it', 'easy', 'to', 'automate', '?', 'all', 'your', 'software', 'workflows', '!']\n"
     ]
    }
   ],
   "source": [
    "result1 = word_tokenize(raw_text1)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2c9c457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GitHub', 'Actions', 'makes', 'it', 'easy', 'to', 'automate', 'all', 'your', 'software', 'workflows']\n"
     ]
    }
   ],
   "source": [
    "result2 = word_tokenize(raw_text2)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2364de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_raw = [raw_text1,raw_text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "660d8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_result=sent_tokenize(raw_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "115c45f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nIn Korea, more than half of residents speak Korean?',\n",
       " 'GitHub Actions makes it easy to automate?',\n",
       " 'all your software workflows!']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4aaf2b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9459fd6a",
   "metadata": {},
   "source": [
    "## 여러 문장에 토큰 추출\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e2d52776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent => \n",
      "In Korea, more than half of residents speak Korean?\n",
      "GitHub Actions makes it easy to automate? \n",
      "all your software workflows!\n",
      "\n",
      "['In', 'Korea', ',', 'more', 'than', 'half', 'of', 'residents', 'speak', 'Korean', '?', 'GitHub', 'Actions', 'makes', 'it', 'easy', 'to', 'automate', '?', 'all', 'your', 'software', 'workflows', '!']\n",
      "-----\n",
      "sent => \n",
      "GitHub Actions makes it easy to automate all your software workflows \n",
      "\n",
      "['GitHub', 'Actions', 'makes', 'it', 'easy', 'to', 'automate', 'all', 'your', 'software', 'workflows']\n",
      "-----\n",
      "------\n",
      "[['GitHub', 'Actions', 'makes', 'it', 'easy', 'to', 'automate', 'all', 'your', 'software', 'workflows']]\n"
     ]
    }
   ],
   "source": [
    "# 문장 단위로 추출\n",
    "for sent in text_raw:\n",
    "    total_token=[]\n",
    "    \n",
    "    # 문장에서 추출한 토큰\n",
    "    print(f'sent => {sent}')\n",
    "    sentToken=word_tokenize(sent)\n",
    "    print(sentToken,'-----',sep='\\n')\n",
    "    # 모든 문장의 토큰에 추가\n",
    "    total_token.append(sentToken)\n",
    "print('------')\n",
    "print(total_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de1b3ff",
   "metadata": {},
   "source": [
    "## 한글\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d56e2198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 형태소 분리 객체\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f875650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '은', '월요일', '입니다']\n"
     ]
    }
   ],
   "source": [
    "result=okt.morphs(\"오늘은 월요일입니다\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "89f59455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('은', 'Josa'), ('월요일', 'Noun'), ('이다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분리 후 태깅(Tagging) => 품사\n",
    "result2=okt.pos(\"오늘은 월요일입니다.\", stem=True)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ec360",
   "metadata": {},
   "source": [
    "### [2] 정제 & 정규화\n",
    "---\n",
    "- 불영어 제거 => 노이즈 제거\n",
    "- 텍스트의 동일화\n",
    "    - 대문자 또는 소문자로 통일\n",
    "    - 문장의 길이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04c33b",
   "metadata": {},
   "source": [
    "### [2-1] 불용어(Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bb1f51cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordListCorpusReader in 'C:\\\\Users\\\\ss\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "154b4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fead01cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb2a7d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea4b1b6",
   "metadata": {},
   "source": [
    "### [2-2] 어간 및 표제어 처리\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6b59d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cf93ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstem = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b65cfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('work', 'work', 'work')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('working'), lstem.stem('works'), lstem.stem('worked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b023085a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amus', 'amus')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('amuse'), lstem.stem('amused')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "81991307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('happy', 'happy')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('happy'), lstem.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7955b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어(사전에 등록된 단어 추출)\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "37bebdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0af21815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('work', 'work')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlemma.lemmatize('working','v'), wlemma.lemmatize('worked','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cbcb5a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amuse', 'amuse')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlemma.lemmatize('amusing','v'), wlemma.lemmatize('amused','v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f32dc5",
   "metadata": {},
   "source": [
    "### [3] 텍스트 벡터화\n",
    "---\n",
    "- 텍스트 => 수치화\n",
    "- 희소벡터(OHE)BOW 방식 --> Count기반, TH-IDF 기반\n",
    "- 밀집벡터 : Embedding 방식, Word2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f01e096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94f08b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nIn Korea, more than half of residents speak Korean?\\nGitHub Actions makes it easy to automate? \\nall your software workflows!\\n',\n",
       " '\\nGitHub Actions makes it easy to automate all your software workflows \\n']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=[raw_text1, raw_text2]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "59001445",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0770862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ohe.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6a269eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 18)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 18)\t1\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bb358cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=result.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4ccecdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 20) [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(result.shape, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ec5c1dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 기반\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e545819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus=tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "80f0923e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "42a65e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus=tf_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4b69840b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18641024, 0.18641024, 0.18641024, 0.18641024, 0.18641024,\n",
       "        0.26199309, 0.26199309, 0.18641024, 0.26199309, 0.26199309,\n",
       "        0.18641024, 0.26199309, 0.26199309, 0.26199309, 0.18641024,\n",
       "        0.26199309, 0.26199309, 0.18641024, 0.18641024, 0.18641024],\n",
       "       [0.30151134, 0.30151134, 0.30151134, 0.30151134, 0.30151134,\n",
       "        0.        , 0.        , 0.30151134, 0.        , 0.        ,\n",
       "        0.30151134, 0.        , 0.        , 0.        , 0.30151134,\n",
       "        0.        , 0.        , 0.30151134, 0.30151134, 0.30151134]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f4262160",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "sent='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using \n",
    "any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks \n",
    "between internal pages on the fly.\\ Wiki is unusual among group communication mechanisms in that \n",
    "it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \n",
    "\"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page \n",
    "in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition \n",
    "by nontechnical users.'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4f3da829",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_text = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7c7418a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nsent='Wiki is in Ward is original description: The simplest online database that could possibly work.Wiki is a piece of server software that allows users to freely create and edit Web page content using \\nany Web browser.\",\n",
       " 'Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks \\nbetween internal pages on the fly.\\\\ Wiki is unusual among group communication mechanisms in that \\nit allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \\n\"open editing\" has some profound and subtle effects on Wiki usage.',\n",
       " \"Allowing everyday users to create and edit any page \\nin a Web site is exciting in that it encourages democratic use of the Web and promotes content composition \\nby nontechnical users.'\"]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c283c5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "27c5ae8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stopwords=nltk.corpus.stopwords.words('english')\n",
    "en_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bfc93ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"sent='Wiki\", 'Ward', 'original', 'description', ':', 'The', 'simplest', 'online', 'database', 'could', 'possibly', 'work.Wiki', 'piece', 'server', 'software', 'allows', 'users', 'freely', 'create', 'edit', 'Web', 'page', 'content', 'using', 'Web', 'browser', '.', 'Wiki', 'supports', 'hyperlinks', 'simple', 'text', 'syntax', 'creating', 'new', 'pages', 'crosslinks', 'internal', 'pages', 'fly.\\\\', 'Wiki', 'unusual', 'among', 'group', 'communication', 'mechanisms', 'allows', 'organization', 'contributions', 'edited', 'addition', 'content', 'itself.Like', 'many', 'simple', 'concepts', ',', \"''\", 'open', 'editing', \"''\", 'profound', 'subtle', 'effects', 'Wiki', 'usage', '.', 'Allowing', 'everyday', 'users', 'create', 'edit', 'page', 'Web', 'site', 'exciting', 'encourages', 'democratic', 'use', 'Web', 'promotes', 'content', 'composition', 'nontechnical', 'users', '.', \"'\"] 87\n"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "for sent in sen_text:\n",
    "    word = word_tokenize(sent)\n",
    "\n",
    "    for w in word:\n",
    "        if w not in en_stopwords:\n",
    "            result.append(w)\n",
    "print(result, len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b30305",
   "metadata": {},
   "source": [
    "## Tokenizer 객체 생성\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a6fe2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as th\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a2179144",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "sent='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using \n",
    "any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks \n",
    "between internal pages on the fly.\\ Wiki is unusual among group communication mechanisms in that \n",
    "it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \n",
    "\"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page \n",
    "in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition \n",
    "by nontechnical users.'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e22350cf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sent', \"'wiki\", 'is', 'in', 'ward', 'is', 'original', 'description', 'the', 'simplest', 'online', 'database', 'that', 'could', 'possibly', 'work', 'wiki', 'is', 'a', 'piece', 'of', 'server', 'software', 'that', 'allows', 'users', 'to', 'freely', 'create', 'and', 'edit', 'web', 'page', 'content', 'using', 'any', 'web', 'browser', 'wiki', 'supports', 'hyperlinks', 'and', 'has', 'a', 'simple', 'text', 'syntax', 'for', 'creating', 'new', 'pages', 'and', 'crosslinks', 'between', 'internal', 'pages', 'on', 'the', 'fly', 'wiki', 'is', 'unusual', 'among', 'group', 'communication', 'mechanisms', 'in', 'that', 'it', 'allows', 'the', 'organization', 'of', 'contributions', 'to', 'be', 'edited', 'in', 'addition', 'to', 'the', 'content', 'itself', 'like', 'many', 'simple', 'concepts', 'open', 'editing', 'has', 'some', 'profound', 'and', 'subtle', 'effects', 'on', 'wiki', 'usage', 'allowing', 'everyday', 'users', 'to', 'create', 'and', 'edit', 'any', 'page', 'in', 'a', 'web', 'site', 'is', 'exciting', 'in', 'that', 'it', 'encourages', 'democratic', 'use', 'of', 'the', 'web', 'and', 'promotes', 'content', 'composition', 'by', 'nontechnical', 'users', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "# 토큰으로 나누기\n",
    "tokens=text_to_word_sequence(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ee859e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "myToken=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3e03eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "myToken.fit_on_texts(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bd79ac65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('sent', 1), (\"'wiki\", 1), ('is', 5), ('in', 5), ('ward', 1), ('original', 1), ('description', 1), ('the', 5), ('simplest', 1), ('online', 1), ('database', 1), ('that', 4), ('could', 1), ('possibly', 1), ('work', 1), ('wiki', 4), ('a', 3), ('piece', 1), ('of', 3), ('server', 1), ('software', 1), ('allows', 2), ('users', 3), ('to', 4), ('freely', 1), ('create', 2), ('and', 6), ('edit', 2), ('web', 4), ('page', 2), ('content', 3), ('using', 1), ('any', 2), ('browser', 1), ('supports', 1), ('hyperlinks', 1), ('has', 2), ('simple', 2), ('text', 1), ('syntax', 1), ('for', 1), ('creating', 1), ('new', 1), ('pages', 2), ('crosslinks', 1), ('between', 1), ('internal', 1), ('on', 2), ('fly', 1), ('unusual', 1), ('among', 1), ('group', 1), ('communication', 1), ('mechanisms', 1), ('it', 2), ('organization', 1), ('contributions', 1), ('be', 1), ('edited', 1), ('addition', 1), ('itself', 1), ('like', 1), ('many', 1), ('concepts', 1), ('open', 1), ('editing', 1), ('some', 1), ('profound', 1), ('subtle', 1), ('effects', 1), ('usage', 1), ('allowing', 1), ('everyday', 1), ('site', 1), ('exciting', 1), ('encourages', 1), ('democratic', 1), ('use', 1), ('promotes', 1), ('composition', 1), ('by', 1), ('nontechnical', 1), (\"'\", 1)])\n"
     ]
    }
   ],
   "source": [
    "print(myToken.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "df4933ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'is': 2, 'in': 3, 'the': 4, 'that': 5, 'wiki': 6, 'to': 7, 'web': 8, 'a': 9, 'of': 10, 'users': 11, 'content': 12, 'allows': 13, 'create': 14, 'edit': 15, 'page': 16, 'any': 17, 'has': 18, 'simple': 19, 'pages': 20, 'on': 21, 'it': 22, 'sent': 23, \"'wiki\": 24, 'ward': 25, 'original': 26, 'description': 27, 'simplest': 28, 'online': 29, 'database': 30, 'could': 31, 'possibly': 32, 'work': 33, 'piece': 34, 'server': 35, 'software': 36, 'freely': 37, 'using': 38, 'browser': 39, 'supports': 40, 'hyperlinks': 41, 'text': 42, 'syntax': 43, 'for': 44, 'creating': 45, 'new': 46, 'crosslinks': 47, 'between': 48, 'internal': 49, 'fly': 50, 'unusual': 51, 'among': 52, 'group': 53, 'communication': 54, 'mechanisms': 55, 'organization': 56, 'contributions': 57, 'be': 58, 'edited': 59, 'addition': 60, 'itself': 61, 'like': 62, 'many': 63, 'concepts': 64, 'open': 65, 'editing': 66, 'some': 67, 'profound': 68, 'subtle': 69, 'effects': 70, 'usage': 71, 'allowing': 72, 'everyday': 73, 'site': 74, 'exciting': 75, 'encourages': 76, 'democratic': 77, 'use': 78, 'promotes': 79, 'composition': 80, 'by': 81, 'nontechnical': 82, \"'\": 83}\n"
     ]
    }
   ],
   "source": [
    "print(myToken.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "31e104ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23], [24], [2], [3], [25], [2], [26], [27], [4], [28], [29], [30], [5], [31], [32], [33], [6], [2], [9], [34], [10], [35], [36], [5], [13], [11], [7], [37], [14], [1], [15], [8], [16], [12], [38], [17], [8], [39], [6], [40], [41], [1], [18], [9], [19], [42], [43], [44], [45], [46], [20], [1], [47], [48], [49], [20], [21], [4], [50], [6], [2], [51], [52], [53], [54], [55], [3], [5], [22], [13], [4], [56], [10], [57], [7], [58], [59], [3], [60], [7], [4], [12], [61], [62], [63], [19], [64], [65], [66], [18], [67], [68], [1], [69], [70], [21], [6], [71], [72], [73], [11], [7], [14], [1], [15], [17], [16], [3], [9], [8], [74], [2], [75], [3], [5], [22], [76], [77], [78], [10], [4], [8], [1], [79], [12], [80], [81], [82], [11], [83]]\n"
     ]
    }
   ],
   "source": [
    "print(myToken.texts_to_sequences(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9541c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "15daa836",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cbdb5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<oov>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8a1159f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe64f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e08d12a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my friend'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4b1c7c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "873975b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n",
      "{'<oov>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "print(test_sequences)\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e48b3b",
   "metadata": {},
   "source": [
    "## One-Hot-Encoding 변환\n",
    "---\n",
    "- sklearn OneHotEncoder 객체 생성\n",
    "- keras 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2928dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "df5d4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sequences)):\n",
    "    sequences[i] = to_categorical(sequences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "06b702f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.]], dtype=float32),\n",
       " array([[0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.]], dtype=float32),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1f542024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "809066ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = pd.read_table('C:/Users/ss/Downloads/example.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "65478fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The main Henry Ford Museum building houses som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Henry Ford Academy is the first charter school...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Freshman meet inside the main museum building ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Henry Ford Learning Institute is using the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The building received the international annual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>See also[edit]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  The main Henry Ford Museum building houses som...\n",
       "1  Henry Ford Academy is the first charter school...\n",
       "2  Freshman meet inside the main museum building ...\n",
       "3  The Henry Ford Learning Institute is using the...\n",
       "4  The building received the international annual...\n",
       "5                                     See also[edit]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "8c75e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = en.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "e19fd9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text = ''\n",
    "for i in en:\n",
    "    en_text = en_text + i[0]\n",
    "en_text = [en_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "bf84dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 1000, oov_token=\"<oov>\")\n",
    "tokenizer.fit_on_texts(en_text)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "d160aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "enSeq = tokenizer.texts_to_sequences(en_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "25fd0b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "eb59f139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 16, 9, 4, 10, 13, 34, 35, 5, 2, 17, 11, 2, 9, 4, 36, 4, 18, 12, 2, 37, 19, 6, 3, 2, 38, 39, 14, 40, 41, 42, 20, 7, 43, 44, 45, 46, 8, 7, 47, 48, 49, 50, 2, 6, 12, 51, 20, 2, 4, 52, 53, 54, 55, 56, 15, 57, 58, 8, 2, 9, 4, 10, 8, 59, 21, 6, 22, 23, 12, 60, 3, 61, 62, 24, 2, 63, 5, 2, 9, 4, 10, 64, 12, 65, 66, 7, 67, 3, 2, 68, 8, 69, 70, 3, 71, 72, 73, 74, 75, 2, 16, 10, 13, 3, 76, 77, 17, 78, 79, 22, 25, 7, 80, 81, 13, 8, 82, 83, 24, 7, 84, 5, 2, 85, 26, 86, 87, 88, 89, 14, 27, 25, 5, 2, 10, 90, 7, 91, 5, 2, 28, 26, 29, 92, 2, 10, 93, 94, 3, 95, 23, 96, 7, 6, 97, 98, 99, 100, 14, 101, 102, 6, 103, 2, 104, 105, 5, 2, 28, 6, 106, 3, 107, 2, 9, 4, 108, 109, 12, 110, 2, 9, 4, 18, 111, 11, 112, 19, 29, 113, 2, 114, 115, 21, 3, 116, 8, 117, 6, 11, 118, 30, 3, 119, 120, 2, 13, 121, 2, 31, 122, 30, 32, 5, 2, 123, 5, 15, 33, 124, 31, 11, 125, 2, 126, 127, 128, 32, 11, 129, 130, 15, 33, 131, 132, 27, 133, 134, 8, 135, 136, 137, 138, 139]]\n"
     ]
    }
   ],
   "source": [
    "print(enSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "b117d9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<oov>': 1, 'the': 2, 'in': 3, 'ford': 4, 'of': 5, 'school': 6, 'a': 7, 'and': 8, 'henry': 9, 'museum': 10, 'for': 11, 'is': 12, 'building': 13, 'to': 14, 'educational': 15, 'main': 16, 'classrooms': 17, 'academy': 18, 'charter': 19, 'by': 20, 'high': 21, 'students': 22, 'it': 23, 'on': 24, 'use': 25, 'village': 26, 'include': 27, 'original': 28, 'schools': 29, 'design': 30, 'international': 31, 'award': 32, 'facilities': 33, 'houses': 34, 'some': 35, 'academyhenry': 36, 'first': 37, 'united': 38, 'states': 39, 'be': 40, 'developed': 41, 'jointly': 42, 'global': 43, 'corporation': 44, 'public': 45, 'education': 46, 'major': 47, 'nonprofit': 48, 'cultural': 49, 'institution': 50, 'sponsored': 51, 'motor': 52, 'company': 53, 'wayne': 54, 'county': 55, 'regional': 56, 'service': 57, 'agency': 58, 'admits': 59, 'located': 60, 'dearborn': 61, 'michigan': 62, 'campus': 63, 'enrollment': 64, 'taken': 65, 'from': 66, 'lottery': 67, 'area': 68, 'totaled': 69, '467': 70, '2010': 71, '1': 72, 'freshman': 73, 'meet': 74, 'inside': 75, 'glass': 76, 'walled': 77, 'while': 78, 'older': 79, 'converted': 80, 'carousel': 81, 'pullman': 82, 'cars': 83, 'siding': 84, 'greenfield': 85, 'railroad': 86, 'classes': 87, 'are': 88, 'expected': 89, 'artifacts': 90, 'tradition': 91, 'when': 92, 'was': 93, 'established': 94, '1929': 95, 'included': 96, 'which': 97, 'served': 98, 'grades': 99, 'kindergarten': 100, 'college': 101, 'trade': 102, 'ages': 103, 'last': 104, 'part': 105, 'closed': 106, '1969': 107, 'learning': 108, 'institute': 109, 'using': 110, 'model': 111, 'further': 112, 'including': 113, 'power': 114, 'house': 115, 'chicago': 116, 'alameda': 117, 'art': 118, 'san': 119, 'antonio': 120, 'received': 121, 'annual': 122, 'council': 123, 'planners': 124, '2001': 125, 'james': 126, 'd': 127, 'macconnell': 128, 'outstanding': 129, 'new': 130, 'notable': 131, 'attendees': 132, 'chris': 133, 'stroud': 134, 'isaac': 135, 'sudut': 136, 'see': 137, 'also': 138, 'edit': 139}\n"
     ]
    }
   ],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "525781e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enSeq = to_categorical(enSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "a4dbcbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "3cfb9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = ['Do you think my dog is amazing?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "01833f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 12, 1]]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf43ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
